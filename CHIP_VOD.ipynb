{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_packs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.decomposition as skd\n",
    "import statsmodels.api as sm\n",
    "import sklearn.manifold as skm\n",
    "import sklearn.cluster as skc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "import ruptures as rpt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import statsmodels.api as sm\n",
    "import sklearn.preprocessing as skp\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "import warnings\n",
    "from scipy.optimize import OptimizeWarning\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer,MinMaxScaler\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 'V:/Departamentos/DVM/Transformação Digital/Projetos/064 - Otimização de Processo com IA no VOD (Ciência de Dados)/VOD_JUL_2024.xlsx'\n",
    "dt = pd.read_excel(dt)\n",
    "dt = pd.DataFrame(dt)\n",
    "dt['Tempo de depressão de vácuo(min)'].replace(',' , '.', inplace = True)\n",
    "dt['Tempo de depressão de vácuo(min)']= pd.to_numeric(dt['Tempo de depressão de vácuo(min)'] , errors = 'coerce')\n",
    "\n",
    "dt['Teor de C após a descarburação(%)'].replace(',' , '.', inplace = True)\n",
    "dt['Teor de C após a descarburação(%)']= pd.to_numeric(dt['Teor de C após a descarburação(%)'] , errors = 'coerce')\n",
    "dt.drop(columns=['DATA', 'Agitação média do TruStir durante o sopro de O2','Agitação média do TruStir durante a descarburação'], inplace=True)\n",
    "dt.replace('Boa', 1 , inplace = True)\n",
    "dt.replace('Ruim', 0 , inplace = True)\n",
    "dt = pd.get_dummies(dt)\n",
    "for i in dt.columns:\n",
    "    median_value = dt[i].median()  \n",
    "    dt[i].fillna(median_value, inplace=True)\n",
    "dt = dt.replace({False: 0, True: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dte = 'V:/Departamentos/DVM/Transformação Digital/Projetos/064 - Otimização de Processo com IA no VOD (Ciência de Dados)/VOD_JUL_2024.xlsx'\n",
    "dte = pd.read_excel(dte)\n",
    "dte = pd.DataFrame(dte)\n",
    "dte['Tempo de depressão de vácuo(min)'].replace(',' , '.', inplace = True)\n",
    "dte['Tempo de depressão de vácuo(min)']= pd.to_numeric(dte['Tempo de depressão de vácuo(min)'] , errors = 'coerce')\n",
    "\n",
    "dte['Teor de C após a descarburação(%)'].replace(',' , '.', inplace = True)\n",
    "dte['Teor de C após a descarburação(%)']= pd.to_numeric(dte['Teor de C após a descarburação(%)'] , errors = 'coerce')\n",
    "dte.drop(columns=['DATA', 'Agitação média do TruStir durante o sopro de O2','Agitação média do TruStir durante a descarburação'], inplace=True)\n",
    "dte.replace('Boa', 1 , inplace = True)\n",
    "dte.replace('Ruim', 0 , inplace = True)\n",
    "dte = pd.get_dummies(dte)\n",
    "for i in dte.columns:\n",
    "    median_value = dte[i].median()  \n",
    "    dte[i].fillna(median_value, inplace=True)\n",
    "dte = dte.replace({False: 0, True: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = 'C:/Users/S409478/Documents/vm_report_vo1727442932637.xls'\n",
    "teste = pd.read_excel(teste)\n",
    "teste = pd.DataFrame(teste)\n",
    "# Extract the first row and set as new header\n",
    "new_header = teste.iloc[0]\n",
    "teste = teste[1:]  # Drop the first row\n",
    "teste.columns = new_header  # Set new header\n",
    "\n",
    "# Reset index\n",
    "teste.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Function to format numbers\n",
    "def insert_decimal(x):\n",
    "    x_str = str(x)\n",
    "    \n",
    "    # Handle cases where there is already a decimal point\n",
    "    if '.' in x_str:\n",
    "        integer_part, decimal_part = x_str.split('.')\n",
    "        # If the integer part is shorter than 2 digits, just return as is\n",
    "        if len(integer_part) <= 3:\n",
    "            return x_str\n",
    "        # Otherwise, insert decimal point after the second-to-last digit\n",
    "        return integer_part[:-3] + '.' + integer_part[-3:] + (f\".{decimal_part}\" if decimal_part else \"\")\n",
    "    \n",
    "    # Handle integer values\n",
    "    if len(x_str) > 3:\n",
    "        return x_str[:-3] + '.' + x_str[-3:]\n",
    "    return x_str\n",
    "\n",
    "# Apply the function to each element in the DataFrame column\n",
    "teste['Tempo abaixo de 10 mbar(min)'] = teste['Tempo abaixo de 10 mbar(min)'].apply(insert_decimal)\n",
    "#teste['Tempo de depressão de vácuo(min)'].replace(',' , '.', inplace = True)\n",
    "teste.drop(columns=['DATA', 'Agitação média do TruStir durante o sopro de O2','Agitação média do TruStir durante a descarburação','AÇO'], inplace=True)\n",
    "#teste = pd.to_numeric(teste, errors='coerce')\n",
    "teste = teste.apply(pd.to_numeric, errors='coerce')\n",
    "teste = teste.dropna(how = 'any')\n",
    "teste = teste.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#priorizar o teor de c após a descarburação na otimização.\n",
    "# optimização:\n",
    "dt['opt'] = 0\n",
    "dt.loc[\n",
    "    (dt['Teor de C após a descarburação(%)'] < 0.009) &\n",
    "(dt['Temperatura do aço após a descarburação(°C)'] > 1680) &\n",
    "(dt['Temperatura do aço após a descarburação(°C)'] < 1700)\n",
    "    ,\n",
    "    'opt'\n",
    "] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#priorizar o teor de c após a descarburação na otimização.\n",
    "# optimização:\n",
    "dte['opt'] = 0\n",
    "dte.loc[\n",
    "    (dte['Teor de C após a descarburação(%)'] < 0.009) &\n",
    "(dte['Temperatura do aço após a descarburação(°C)'] > 1680) &\n",
    "(dte['Temperatura do aço após a descarburação(°C)'] < 1700)\n",
    "    ,\n",
    "    'opt'\n",
    "] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#priorizar o teor de c após a descarburação na otimização.\n",
    "# optimização:\n",
    "teste['opt'] = 0\n",
    "teste.loc[\n",
    "    (teste['Teor de C após a descarburação(%)'] < 0.009) &\n",
    "(teste['Temperatura do aço após a descarburação(°C)'] > 1680) &\n",
    "(teste['Temperatura do aço após a descarburação(°C)'] < 1700)\n",
    "    ,\n",
    "    'opt'\n",
    "] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primeira volta NB / PCA:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE  # Optional if using resampled data\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Required columns for the model\n",
    "required_columns = [\n",
    "    'Teor de C antes do VOD(%)', 'Teor de Cr antes do VOD(%)',\n",
    "    'Teor de Si antes do VOD(%)', 'Teor de Mn antes do VOD(%)',\n",
    "    'Teor de Ni antes do VOD(%)', 'Teor de Mo antes do VOD(%)',\n",
    "    'Temperatura do aço antes do VOD(°C)', 'Peso de aço antes do VOD(kg) ',\n",
    "    'Borda livre da panela(mm)', 'Vida de plug da panela', 'Vida da panela',\n",
    "    'Vazão média de argônio durante o sopro de O2(Nm3/h)',\n",
    "    'Pressão média de vácuo durante o sopro de O2(mbar)',\n",
    "    'Tempo abaixo de 10 mbar(min)', 'Vazão média de argônio durante a descarburação(Nm3/h)',\n",
    "    'Volume de O2 soprado(m3)', 'Temperatura máxima dos gases durante o sopro de O2(°C)',\n",
    "    'Pressão final de vácuo do sopro de O2(mbar)', 'Tempo de depressão de vácuo(min)',\n",
    "    'Pressão final de vácuo na descarburação(mbar)','Tempo total de descarburação(min)'\n",
    "]\n",
    "\n",
    "columns_control = [\n",
    "    'Vazão média de argônio durante o sopro de O2(Nm3/h)',\n",
    "    'Pressão média de vácuo durante o sopro de O2(mbar)',\n",
    "    'Tempo abaixo de 10 mbar(min)', 'Vazão média de argônio durante a descarburação(Nm3/h)',\n",
    "    'Volume de O2 soprado(m3)', 'Temperatura máxima dos gases durante o sopro de O2(°C)',\n",
    "    'Pressão final de vácuo do sopro de O2(mbar)', 'Tempo de depressão de vácuo(min)',\n",
    "    'Pressão final de vácuo na descarburação(mbar)','Tempo total de descarburação(min)'\n",
    "]\n",
    "\n",
    "# Prepare input DataFrame\n",
    "ent = teste[['Teor de C antes do VOD(%)', 'Teor de Cr antes do VOD(%)', \n",
    "             'Teor de Si antes do VOD(%)', 'Teor de Mn antes do VOD(%)',\n",
    "             'Teor de Ni antes do VOD(%)', 'Teor de Mo antes do VOD(%)',\n",
    "             'Temperatura do aço antes do VOD(°C)', 'Peso de aço antes do VOD(kg) ',\n",
    "             'Borda livre da panela(mm)', 'Vida de plug da panela', 'Vida da panela']]\n",
    "\n",
    "#ent_ = ent.sample(n=1)\n",
    "stp = teste.iloc[ent_.index][columns_control].values\n",
    "contr = pd.DataFrame(stp, columns=columns_control)\n",
    "\n",
    "# Combine control and input data\n",
    "contr.reset_index(drop=True, inplace=True)\n",
    "tt2 = pd.concat([ent_, contr], axis=1)\n",
    "tt2.fillna(0, inplace=True)\n",
    "tt2 = pd.DataFrame(tt2.sum()).T\n",
    "\n",
    "# Prepare data for training and testing\n",
    "x_ = dt[required_columns]\n",
    "y_ = dt['opt']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x_scaled = scaler.fit_transform(x_)\n",
    "\n",
    "\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "x_pca = pca.fit_transform(x_)  # Transform training data\n",
    "tt2_pca = pca.transform(tt2)  # Transform the test data\n",
    "\n",
    "# Train Gaussian Naive Bayes on PCA-transformed data\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_pca, y_)\n",
    "\n",
    "# Make predictions on the PCA-transformed test data\n",
    "y_pred = gnb.predict(tt2_pca)\n",
    "y_prob = gnb.predict_proba(tt2_pca)\n",
    "\n",
    "# Display predictions and probabilities\n",
    "print(f\"Prediction: {y_pred}\")\n",
    "print(f\"Sample Index: {ent_.index}\")\n",
    "print(f\"Prediction Probabilities: {y_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHIP, OFF_2:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom regularization to discourage low coefficients\n",
    "def custom_regularization(params, threshold=0.1, lambda_=0.01):\n",
    "    return lambda_ * np.sum(np.clip(np.abs(params) - threshold, 0, None) ** 2)\n",
    "\n",
    "# Huber loss with regularization\n",
    "def huber_loss(params, X, y, delta=1.0, lambda_=0.01, threshold=0.1):\n",
    "    y_pred = 1 / (1 + np.exp(-np.dot(X, params)))\n",
    "    residuals = y - y_pred\n",
    "    abs_residuals = np.abs(residuals)\n",
    "    \n",
    "    loss = np.where(abs_residuals <= delta, 0.5 * residuals**2, delta * (abs_residuals - 0.5 * delta))\n",
    "    regularization_term = custom_regularization(params, threshold, lambda_)\n",
    "    \n",
    "    return (np.sum(loss) / X.shape[0]) + regularization_term\n",
    "\n",
    "# Gradient of Huber loss\n",
    "def huber_loss_grad(params, X, y, delta=1.0, lambda_=0.01, threshold=0.1):\n",
    "    y_pred = 1 / (1 + np.exp(-np.dot(X, params)))\n",
    "    residuals = y - y_pred\n",
    "    abs_residuals = np.abs(residuals)\n",
    "    \n",
    "    grad = -np.dot(X.T, np.where(abs_residuals <= delta, residuals, delta * np.sign(residuals))) / X.shape[0]\n",
    "    reg_grad = lambda_ * np.where(np.abs(params) < threshold, 0, params - threshold * np.sign(params))\n",
    "    \n",
    "    return grad + reg_grad\n",
    "\n",
    "# Constraints for processing\n",
    "constraints = {\n",
    "    'Tempo abaixo de 10 mbar(min)': (9.65, 23),\n",
    "    'Vazão média de argônio durante o sopro de O2(Nm3/h)': (3.52, 6.83),\n",
    "    'Pressão média de vácuo durante o sopro de O2(mbar)': (400, 693.32),\n",
    "    'Volume de O2 soprado(m3)': (115.39, 248.45),\n",
    "    'Vazão média de argônio durante a descarburação(Nm3/h)': (4.55, 7.87)\n",
    "}\n",
    "\n",
    "# Initialize control variables\n",
    "condition_met = False\n",
    "start_time = time.time()\n",
    "time_limit = 10  # Seconds\n",
    "iteration = 0\n",
    "max_probability = 0  # Track the highest predicted probability\n",
    "\n",
    "while not condition_met:\n",
    "    try:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time > time_limit:\n",
    "            print(\"Time limit exceeded. Stopping the loop.\")\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "        print(f\"Iteration: {iteration}, Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # Combine data and fill missing values\n",
    "        tt2 = pd.concat([ent_, contr], axis=1).fillna(0)\n",
    "        tt2 = pd.DataFrame(tt2.sum()).T\n",
    "\n",
    "        # Apply constraints to tt2\n",
    "        for col, (a_min, a_max) in constraints.items():\n",
    "            if col in tt2.columns:\n",
    "                tt2[col] = np.clip(tt2[col], a_min, a_max)\n",
    "\n",
    "        # Define required columns for model input\n",
    "        required_columns = [\n",
    "            'Teor de C antes do VOD(%)', 'Teor de Cr antes do VOD(%)',\n",
    "            'Teor de Si antes do VOD(%)', 'Teor de Mn antes do VOD(%)',\n",
    "            'Teor de Ni antes do VOD(%)', 'Teor de Mo antes do VOD(%)',\n",
    "            'Temperatura do aço antes do VOD(°C)', 'Peso de aço antes do VOD(kg) ',\n",
    "            'Borda livre da panela(mm)', 'Vida de plug da panela', 'Vida da panela',\n",
    "            'Vazão média de argônio durante o sopro de O2(Nm3/h)',\n",
    "            'Pressão média de vácuo durante o sopro de O2(mbar)',\n",
    "            'Tempo abaixo de 10 mbar(min)', 'Vazão média de argônio durante a descarburação(Nm3/h)',\n",
    "            'Volume de O2 soprado(m3)'\n",
    "        ]\n",
    "\n",
    "        # Align test data columns with training data\n",
    "        tt2_test = tt2[required_columns]\n",
    "        x_ = dt[required_columns]\n",
    "        y_ = dt['opt']\n",
    "\n",
    "        # Train/test split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_, y_, test_size=0.8)\n",
    "\n",
    "        # Apply PCA to reduce dimensionality\n",
    "        pca = PCA(n_components=0.95)\n",
    "        x_train_pca = pca.fit_transform(x_train)\n",
    "        tt2_pca = pca.transform(tt2_test)\n",
    "\n",
    "        # Train Gaussian Naive Bayes on PCA-transformed data\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(x_train_pca, y_train)\n",
    "\n",
    "        # Predict on PCA-transformed test data\n",
    "        y_pred = gnb.predict(tt2_pca)\n",
    "        y_prob = gnb.predict_proba(tt2_pca)\n",
    "\n",
    "        # Store the maximum probability so far\n",
    "        max_probability = max(max_probability, np.max(y_prob[:, 1]))\n",
    "\n",
    "        # Display predictions\n",
    "        tt2['Predicted_Probability'] = y_prob[:, 1]\n",
    "        tt2['Predicted_Class'] = y_pred\n",
    "\n",
    "        print(\"Predictions on tt2:\")\n",
    "        print(tt2[['Predicted_Probability', 'Predicted_Class']])\n",
    "\n",
    "        # Fit logistic regression with robust standard errors\n",
    "        logit_model = sm.Logit(y_train, x_train)\n",
    "        results = logit_model.fit(method='bfgs', cov_type='HC3')\n",
    "\n",
    "        # Extract and process coefficients\n",
    "        coefs = results.params\n",
    "        exp_coeff = np.exp(coefs) / (np.exp(coefs) + 1)\n",
    "        exp_coeff_df = pd.DataFrame({'exp_coeff': exp_coeff})\n",
    "\n",
    "        # Filter significant variables\n",
    "        summary = results.summary().tables[1].as_html()\n",
    "        df_results = pd.read_html(summary, header=0, index_col=0)[0]\n",
    "        df_results['exp_coeff'] = exp_coeff_df\n",
    "        significant_vars = df_results[df_results['P>|z|'] < 0.05]\n",
    "\n",
    "        # Calculate corrections\n",
    "        results_list = []\n",
    "        for col in significant_vars.index:\n",
    "            value = significant_vars.loc[col, 'exp_coeff']\n",
    "            correction = tt2_test[col].median() if col in constraints else 0\n",
    "            result = (value - 0.5) * correction if value != 0.5 else 0\n",
    "            results_list.append({'Column': col, 'Correction': result})\n",
    "\n",
    "        corrections_df = pd.DataFrame(results_list).T\n",
    "        corrections_df.columns = corrections_df.iloc[0]\n",
    "        corrections_df = corrections_df[1:]\n",
    "\n",
    "        # Update control DataFrame\n",
    "        common_columns = list(set(constraints) & set(corrections_df.columns))\n",
    "        combined_df = pd.concat([contr[common_columns], corrections_df[common_columns]], ignore_index=True)\n",
    "        final_df = combined_df.sum().reset_index().T\n",
    "        final_df.columns = final_df.iloc[0]\n",
    "        final_df = final_df[1:]\n",
    "        contr.update(final_df)\n",
    "\n",
    "        # Clear memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Check termination condition\n",
    "        if np.all(tt2['Predicted_Probability'] >= 0.7) and np.all(tt2['Predicted_Class'] == 1):\n",
    "            condition_met = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        continue\n",
    "\n",
    "# Print the maximum probability after loop terminates\n",
    "print(f\"The highest predicted probability reached: {max_probability:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
