{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9539f9-be5d-41ba-825b-01d799579ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_packs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.decomposition as skd\n",
    "import statsmodels.api as sm\n",
    "import sklearn.manifold as skm\n",
    "import sklearn.cluster as skc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "import ruptures as rpt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import statsmodels.api as sm\n",
    "import sklearn.preprocessing as skp\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "import warnings\n",
    "from scipy.optimize import OptimizeWarning\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b38d9a-0176-413b-9970-8b5579f7ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 'V:/Departamentos/DVM/Transformação Digital/Projetos/064 - Otimização de Processo com IA no VOD (Ciência de Dados)/VOD_JUL_2024.xlsx'\n",
    "dt = pd.read_excel(dt)\n",
    "dt = pd.DataFrame(dt)\n",
    "dt['Tempo de depressão de vácuo(min)'].replace(',' , '.', inplace = True)\n",
    "dt['Tempo de depressão de vácuo(min)']= pd.to_numeric(dt['Tempo de depressão de vácuo(min)'] , errors = 'coerce')\n",
    "\n",
    "dt['Teor de C após a descarburação(%)'].replace(',' , '.', inplace = True)\n",
    "dt['Teor de C após a descarburação(%)']= pd.to_numeric(dt['Teor de C após a descarburação(%)'] , errors = 'coerce')\n",
    "dt.drop(columns=['DATA', 'Agitação média do TruStir durante o sopro de O2','Agitação média do TruStir durante a descarburação'], inplace=True)\n",
    "dt.replace('Boa', 1 , inplace = True)\n",
    "dt.replace('Ruim', 0 , inplace = True)\n",
    "dt = pd.get_dummies(dt)\n",
    "for i in dt.columns:\n",
    "    median_value = dt[i].median()  \n",
    "    dt[i].fillna(median_value, inplace=True)\n",
    "dt = dt.replace({False: 0, True: 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26827d-4ac3-4e9d-9eeb-0d93faa199ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalization\n",
    "scaler = skp.RobustScaler()\n",
    "dt_k = scaler.fit_transform(dt)\n",
    "kmeans = KMeans(n_clusters= 400, random_state=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(dt_k)\n",
    "\n",
    "# Get cluster centers and labels\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "dt['cluster'] = labels \n",
    "tag =pd.DataFrame(dt['cluster'])\n",
    "tag = pd.get_dummies(dt['cluster'], prefix='cluster')\n",
    "tag.replace({False: 0, True: 1})\n",
    "## opt\n",
    "dt['opt'] = 0\n",
    "dt.loc[\n",
    "    (dt['Teor de C após a descarburação(%)'] < 0.008) &\n",
    "(dt['Temperatura do aço após a descarburação(°C)'] > 1680) \n",
    "    ,\n",
    "    'opt'\n",
    "] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c5c38-8710-426b-b463-e42f72485bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#priorizar o teor de c após a descarburação na otimização.\n",
    "# optimização:\n",
    "dt['opt'] = 0\n",
    "dt.loc[\n",
    "    (dt[''] <= 0.008) &\n",
    "(dt[''] >= 1680) &\n",
    "(dt[''] <= 1760)\n",
    "    ,\n",
    "    'opt'\n",
    "] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec834d-fe2e-4d4e-acab-126ff908dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Logit:\n",
    "x_ = dt[[ ]]\n",
    "x_x = tag[tag.columns]\n",
    "\n",
    "\n",
    "y_ = dt['opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede721b4-75f7-46a5-a3e5-91f5507fe561",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_x, y_, test_size=0.5)\n",
    "# Decision tree classifier:\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(x_x, y_)\n",
    "\n",
    "result = permutation_importance(clf, x_x, y_, n_repeats=10, random_state=42)\n",
    "importance = result.importances_mean\n",
    "importance = pd.DataFrame(importance)\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = [dt.columns.to_numpy]\n",
    "impor_var = importance[importance[0] > importance[0].mean()]\n",
    "impor_var_new = impor_var.reset_index().rename(columns={'index': 'old_index'})\n",
    "\n",
    "model= sm.Logit(y_,x_)\n",
    "co =model.fit().params\n",
    "coe = pd.DataFrame(co)\n",
    "coe.columns = ['coeff']\n",
    "exp_coeff =   np.exp(coe)/(np.exp(coe) + 1) \n",
    "exp_coeff = pd.DataFrame(exp_coeff)\n",
    "exp_coeff.columns = ['exp_coeff']\n",
    "exp_coeff\n",
    "results = model.fit(method = 'bfgs',cov_type='HC3' )\n",
    "results_summary = results.summary()\n",
    "results_as_html = results_summary.tables[1].as_html()\n",
    "df_results = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "df_results['exp_coeff'] = exp_coeff\n",
    "df_results\n",
    "impact_var = df_results.loc[(df_results['P>|z|'] < 0.05 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a23e9d-a040-4b32-b69e-cc7738295587",
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_tab = dt.groupby(['cluster']).median()\n",
    "clust_test = clus_tab.loc[impor_var_new['old_index']]\n",
    "clust_test_test = clust_test[(clust_test['opt'] == clust_test['opt'].max())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d4601-8386-4c6e-9dbf-35e8659718dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_test_test[[]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ae6ff-1e1b-4710-9e63-8cb5b67e5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_test_test[[]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d678f-71ec-4d89-8b80-238054a534bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.215138 , 450.263303 ,  17.880523 ,    6.347615,  178.655963, 1699.509174\n",
    "4.9000 , 0 , 17.8830 , 6.4500 , 187.0000,1684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457ec2d-01fa-48ff-9368-2f7842f3d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## primaira volta: # inclui a temperatura no final da corrida\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "columns_ = [\n",
    "   ]\n",
    "   \n",
    "stp = [[5.215138 , 450.263303 ,  17.880523 ,    6.347615,  178.655963, 1699.509174]]\n",
    "contr = pd.DataFrame(stp, columns = columns_)\n",
    "# controle df:\n",
    "ent = dt[['']]\n",
    "ent_ = ent.sample(n=1)\n",
    "contr.reset_index(drop=True, inplace=False)\n",
    "tt2 = pd.concat([ent_,contr], axis=1)\n",
    "tt2.fillna(0)\n",
    "tt2 =pd.DataFrame(tt2.sum())\n",
    "tt2 = tt2.T\n",
    "tt2_test = tt2[[\n",
    "                ]]\n",
    "x_ = dt[[]]\n",
    "y_ = dt['opt']\n",
    "#tt2_test.columns = x_train_3.columns\n",
    "x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(x_,y_, test_size = 0.9)\n",
    "model = LogisticRegression(penalty='l2', solver='liblinear',class_weight= 'balanced')\n",
    "model.fit(x_, y_)\n",
    "y_pred2 = model.predict(tt2)\n",
    "y_pred_prob2 = model.predict_proba(tt2)\n",
    "y_pred_score2 = model.score(x_, y_)\n",
    "###\n",
    "## inferência causal:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "model= sm.Logit(y_train_3, x_train_3)\n",
    "co =model.fit(method = 'bfgs',cov_type='HC3').params\n",
    "coe = pd.DataFrame(co)\n",
    "coe.columns = ['coeff']\n",
    "exp_coeff =   np.exp(coe)/(np.exp(coe) + 1) \n",
    "exp_coeff = pd.DataFrame(exp_coeff)\n",
    "exp_coeff.columns = ['exp_coeff']\n",
    "exp_coeff\n",
    "results = model.fit(method = 'bfgs',cov_type='HC3')\n",
    "results_summary = results.summary()\n",
    "results_as_html = results_summary.tables[1].as_html()\n",
    "df_results = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "df_results['exp_coeff'] = exp_coeff\n",
    "impact_var2 = df_results.loc[(df_results['P>|z|'] < 0.05 )]\n",
    "###\n",
    "results_list = []\n",
    "\n",
    "# Iterate over each index in impact_var2\n",
    "for i in impact_var2.index:\n",
    "    value = impact_var2.loc[i, 'exp_coeff']\n",
    "    \n",
    "    # Check if the column exists in contr\n",
    "    if i in contr.columns:\n",
    "        correction = tt2_test[i].mean()\n",
    "    \n",
    "        # Calculate the result based on the value\n",
    "        if value < 0.5:\n",
    "            result = (value - 0.5) * correction\n",
    "        elif value > 0.5:\n",
    "            result = (value - 0.5) * correction\n",
    "        else:\n",
    "            result = 0  # Handle the case where value == 0.5\n",
    "\n",
    "        # Append result to the list\n",
    "        results_list.append({'Column': i, 'Alteração': result})\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df_ = pd.DataFrame(results_list)\n",
    "\n",
    "# Filter results_df_ to include only columns present in contr\n",
    "results_df_\n",
    "\n",
    "####\n",
    "dd = results_df_.T\n",
    "new_header = dd.iloc[0]  # The first row\n",
    "\n",
    "# Step 2: Create a new DataFrame with the new headers\n",
    "dd = dd[1:]  # Remove the first row from the DataFrame\n",
    "dd.columns = new_header  # Set the new header as column names\n",
    "\n",
    "# Reset index if needed\n",
    "dd.reset_index(drop=True, inplace=True)\n",
    "##\n",
    "common_columns = list(set(contr.columns) & set(dd.columns))\n",
    "contr_common = contr[common_columns]\n",
    "dd_common = dd[common_columns]\n",
    "\n",
    "# Step 3: Concatenate the DataFrames and sum the values\n",
    "# Concatenate DataFrames\n",
    "combined_df = pd.concat([contr_common, dd_common], ignore_index=True)\n",
    "\n",
    "# Sum the values for the common columns\n",
    "final = combined_df.sum().reset_index()\n",
    "tt = final.T\n",
    "new_header = tt.iloc[0]  # The first row\n",
    "\n",
    "# Step 2: Create a new DataFrame with the new headers\n",
    "tt = tt[1:]  # Remove the first row from the DataFrame\n",
    "tt.columns = new_header  # Set the new header as column names\n",
    "\n",
    "# Reset index if needed\n",
    "tt.reset_index(drop=True, inplace=True)\n",
    "###\n",
    "contr.reset_index(drop = True)\n",
    "tt.reset_index(drop = True)\n",
    "\n",
    "# Step 2: Identify common columns\n",
    "common_columns = contr.columns.intersection(tt.columns)\n",
    "\n",
    "# Step 3: Update values in df_main for the common columns\n",
    "contr.update(tt[common_columns])\n",
    "\n",
    "# Reset index if needed\n",
    "contr.reset_index(inplace=True)\n",
    "###\n",
    "contr.drop(columns=['index'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b679d42-9844-44de-953a-edf860d5f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b853772-e16f-433e-91de-c982941c72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ddd66-b7f2-4d2d-bde2-a117a0775ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930dbaf-1213-4352-8f94-00eb264b8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loop variables\n",
    "warnings.filterwarnings('ignore')\n",
    "condition_met = False\n",
    "start_time = time.time()\n",
    "time_limit = 300 \n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while not condition_met:\n",
    "    try:\n",
    "        # Track elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time > time_limit:\n",
    "            print(\"Time limit exceeded. Stopping the loop.\")\n",
    "            break\n",
    "        \n",
    "        iteration += 1\n",
    "        print(f\"Iteration: {iteration}, Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # Reset index for contr DataFrame\n",
    "        contr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Concatenate and process DataFrames\n",
    "        tt2 = pd.concat([ent_, contr], axis=1)\n",
    "        tt2.fillna(0, inplace=True)\n",
    "        tt2 = pd.DataFrame(tt2.sum()).T\n",
    "\n",
    "        tt2_test = tt2[[\n",
    "                ]]\n",
    "\n",
    "        x_ = dt[[]]\n",
    "        y_ = dt['opt']\n",
    "\n",
    "        x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(x_, y_, test_size=0.9)\n",
    "        model = LogisticRegression(penalty='l2', solver='liblinear', class_weight='balanced')\n",
    "        model.fit(x_train_3, y_train_3)\n",
    "        y_pred2 = model.predict(tt2)\n",
    "        y_pred_prob2 = model.predict_proba(tt2)[:, 1]  # Get the probability of the positive class\n",
    "        y_pred_score2 = model.score(x_test_3, y_test_3)\n",
    "\n",
    "        # Inferência causal\n",
    "        model = sm.Logit(y_train_3, x_train_3)\n",
    "        co = model.fit(method='bfgs', cov_type='HC3').params\n",
    "        coe = pd.DataFrame(co)\n",
    "        coe.columns = ['coeff']\n",
    "        exp_coeff = np.exp(coe) / (np.exp(coe) + 1)\n",
    "        exp_coeff = pd.DataFrame(exp_coeff)\n",
    "        exp_coeff.columns = ['exp_coeff']\n",
    "        results = model.fit(method='bfgs', cov_type='HC3')\n",
    "        results_summary = results.summary()\n",
    "        results_as_html = results_summary.tables[1].as_html()\n",
    "        df_results = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "        df_results['exp_coeff'] = exp_coeff\n",
    "        impact_var2 = df_results.loc[df_results['P>|z|'] < 0.05]\n",
    "\n",
    "        results_list = []\n",
    "        for i in impact_var2.index:\n",
    "            value = impact_var2.loc[i, 'exp_coeff']\n",
    "\n",
    "            if i in contr.columns:\n",
    "                correction = tt2_test[i].mean()\n",
    "\n",
    "                if value < 0.5:\n",
    "                    result = (value - 0.5) * correction\n",
    "                elif value > 0.5:\n",
    "                    result = (value - 0.5) * correction\n",
    "                else:\n",
    "                    result = 0\n",
    "\n",
    "                results_list.append({'Column': i, 'Alteração': result})\n",
    "\n",
    "        results_df_ = pd.DataFrame(results_list)\n",
    "        dd = results_df_.T\n",
    "        new_header = dd.iloc[0]\n",
    "        dd = dd[1:]\n",
    "        dd.columns = new_header\n",
    "        dd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        common_columns = list(set(contr.columns) & set(dd.columns))\n",
    "        contr_common = contr[common_columns]\n",
    "        dd_common = dd[common_columns]\n",
    "\n",
    "        combined_df = pd.concat([contr_common, dd_common], ignore_index=True)\n",
    "        final = combined_df.sum().reset_index()\n",
    "        tt = final.T\n",
    "        new_header = tt.iloc[0]\n",
    "        tt = tt[1:]\n",
    "        tt.columns = new_header\n",
    "        tt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        contr.reset_index(drop=True, inplace=True)\n",
    "        tt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        common_columns = contr.columns.intersection(tt.columns)\n",
    "        contr.update(tt[common_columns])\n",
    "        contr.reset_index(inplace=True)\n",
    "        contr.drop(columns=['index'], inplace=True)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        # Check if the condition is met\n",
    "        if np.all(y_pred_prob2 >= 0.99) and np.all(y_pred2 == 1):\n",
    "            condition_met = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Handle or log the error as needed\n",
    "        continue\n",
    "\n",
    "# After the loop ends, display the results\n",
    "print(\"Final Results:\")\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Accuracy Score: {y_pred_score2:.4f}\")\n",
    "\n",
    "print(\"\\nPredicted Classes:\")\n",
    "print(pd.DataFrame({'Predicted Class': y_pred2}))\n",
    "\n",
    "print(\"\\nPredicted Probabilities:\")\n",
    "print(pd.DataFrame({'Predicted Probability': y_pred_prob2}))\n",
    "\n",
    "print(\"\\nResults DataFrame:\")\n",
    "print(results_df_)\n",
    "\n",
    "print(\"\\nUpdated 'contr' DataFrame:\")\n",
    "print(contr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c914e9-662c-4707-bd07-2d33c951e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcef597-2b66-4d16-a316-6f76135b320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbe0d6-dadf-4507-8e3c-b33d83ebdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83704dfc-3ce0-4c7b-89e7-adbf523bd0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt2[columns_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbc09d-f3a8-4dd1-a1a3-4ba0d6c8b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "stp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
